{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Z92adMb9gSaT",
        "outputId": "a91435d4-c778-483d-dbe6-d21d72e78ea1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧠 CPU: float32 ON\n",
            "📦 Sube el ZIP para *validas*\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9603ac02-c458-456f-bbd7-28529a10e0cd\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9603ac02-c458-456f-bbd7-28529a10e0cd\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving validas.zip to validas.zip\n",
            "📦 Sube el ZIP para *no_validas*\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-195452fe-6187-420d-b7a3-cb21b9f368b3\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-195452fe-6187-420d-b7a3-cb21b9f368b3\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving no_validas.zip to no_validas.zip\n",
            "📦 validas movidas: 1104  |  no_validas movidas: 338\n",
            "🧼 Sanitizando 'validas'…\n",
            "   ✔ convertidos: 0, re-salvados jpg: 1104, fallidos: 0\n",
            "🧼 Sanitizando 'no_validas'…\n",
            "   ✔ convertidos: 0, re-salvados jpg: 338, fallidos: 0\n",
            "📊 Dataset → no_validas=338 | validas=1104\n",
            "✂️ Split → train: no_validas=271 validas=884 | val: no_validas=67 validas=220\n",
            "⚖️ Train balanceado (none) → no_validas=271 | validas=884\n",
            "🔼 Cargando MobileNetV2 (ImageNet)…\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3028705419.py:203: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "  base = mobilenet_v2.MobileNetV2(include_top=False, weights=\"imagenet\", input_tensor=inputs, alpha=1.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "⚖️ class_weights: {1: 2.1309963099630997, 0: 0.6532805429864253}\n",
            "\n",
            "🚀 Fase 1: entrenando cabezal (backbone congelado)\n",
            "Epoch 1/8\n",
            "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 439ms/step - auc_pr: 0.2842 - auc_roc: 0.6125 - bin_acc: 0.6390 - loss: 0.8052 - precision: 0.2725 - recall: 0.4562\n",
            "Epoch 1: val_loss improved from inf to 0.41972, saving model to ./dataset/best.keras\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 763ms/step - auc_pr: 0.2896 - auc_roc: 0.6156 - bin_acc: 0.6389 - loss: 0.8032 - precision: 0.2760 - recall: 0.4627 - val_auc_pr: 0.6854 - val_auc_roc: 0.8607 - val_bin_acc: 0.8118 - val_loss: 0.4197 - val_precision: 0.5823 - val_recall: 0.6866 - learning_rate: 0.0010\n",
            "Epoch 2/8\n",
            "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 492ms/step - auc_pr: 0.6007 - auc_roc: 0.8369 - bin_acc: 0.7758 - loss: 0.5228 - precision: 0.5204 - recall: 0.6882\n",
            "Epoch 2: val_loss improved from 0.41972 to 0.37287, saving model to ./dataset/best.keras\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 596ms/step - auc_pr: 0.6015 - auc_roc: 0.8374 - bin_acc: 0.7762 - loss: 0.5221 - precision: 0.5207 - recall: 0.6908 - val_auc_pr: 0.7657 - val_auc_roc: 0.9016 - val_bin_acc: 0.8293 - val_loss: 0.3729 - val_precision: 0.6000 - val_recall: 0.8060 - learning_rate: 0.0010\n",
            "Epoch 3/8\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 526ms/step - auc_pr: 0.7131 - auc_roc: 0.8850 - bin_acc: 0.7991 - loss: 0.4357 - precision: 0.5379 - recall: 0.8037\n",
            "Epoch 3: val_loss improved from 0.37287 to 0.32642, saving model to ./dataset/best.keras\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 659ms/step - auc_pr: 0.7133 - auc_roc: 0.8851 - bin_acc: 0.7992 - loss: 0.4354 - precision: 0.5383 - recall: 0.8035 - val_auc_pr: 0.7976 - val_auc_roc: 0.9137 - val_bin_acc: 0.8467 - val_loss: 0.3264 - val_precision: 0.6420 - val_recall: 0.7761 - learning_rate: 0.0010\n",
            "Epoch 4/8\n",
            "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 467ms/step - auc_pr: 0.7803 - auc_roc: 0.9058 - bin_acc: 0.8345 - loss: 0.4036 - precision: 0.6211 - recall: 0.8218\n",
            "Epoch 4: val_loss improved from 0.32642 to 0.30924, saving model to ./dataset/best.keras\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 597ms/step - auc_pr: 0.7806 - auc_roc: 0.9062 - bin_acc: 0.8353 - loss: 0.4025 - precision: 0.6220 - recall: 0.8219 - val_auc_pr: 0.8104 - val_auc_roc: 0.9219 - val_bin_acc: 0.8537 - val_loss: 0.3092 - val_precision: 0.6623 - val_recall: 0.7612 - learning_rate: 0.0010\n",
            "Epoch 5/8\n",
            "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 465ms/step - auc_pr: 0.8223 - auc_roc: 0.9471 - bin_acc: 0.8751 - loss: 0.2863 - precision: 0.6643 - recall: 0.8562\n",
            "Epoch 5: val_loss did not improve from 0.30924\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 562ms/step - auc_pr: 0.8219 - auc_roc: 0.9467 - bin_acc: 0.8745 - loss: 0.2879 - precision: 0.6642 - recall: 0.8564 - val_auc_pr: 0.8111 - val_auc_roc: 0.9236 - val_bin_acc: 0.8432 - val_loss: 0.3355 - val_precision: 0.6222 - val_recall: 0.8358 - learning_rate: 0.0010\n",
            "Epoch 6/8\n",
            "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 451ms/step - auc_pr: 0.8031 - auc_roc: 0.9269 - bin_acc: 0.8631 - loss: 0.3556 - precision: 0.6601 - recall: 0.8688\n",
            "Epoch 6: val_loss did not improve from 0.30924\n",
            "\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 583ms/step - auc_pr: 0.8035 - auc_roc: 0.9266 - bin_acc: 0.8625 - loss: 0.3561 - precision: 0.6591 - recall: 0.8675 - val_auc_pr: 0.8156 - val_auc_roc: 0.9251 - val_bin_acc: 0.8432 - val_loss: 0.3605 - val_precision: 0.6146 - val_recall: 0.8806 - learning_rate: 0.0010\n",
            "Epoch 7/8\n",
            "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 454ms/step - auc_pr: 0.8055 - auc_roc: 0.9277 - bin_acc: 0.8500 - loss: 0.3452 - precision: 0.6326 - recall: 0.8795\n",
            "Epoch 7: val_loss improved from 0.30924 to 0.28399, saving model to ./dataset/best.keras\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 572ms/step - auc_pr: 0.8076 - auc_roc: 0.9289 - bin_acc: 0.8514 - loss: 0.3420 - precision: 0.6350 - recall: 0.8806 - val_auc_pr: 0.8214 - val_auc_roc: 0.9276 - val_bin_acc: 0.8606 - val_loss: 0.2840 - val_precision: 0.7015 - val_recall: 0.7015 - learning_rate: 5.0000e-04\n",
            "Epoch 8/8\n",
            "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 463ms/step - auc_pr: 0.8513 - auc_roc: 0.9450 - bin_acc: 0.8885 - loss: 0.3087 - precision: 0.7248 - recall: 0.8561\n",
            "Epoch 8: val_loss did not improve from 0.28399\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 561ms/step - auc_pr: 0.8511 - auc_roc: 0.9451 - bin_acc: 0.8881 - loss: 0.3078 - precision: 0.7234 - recall: 0.8569 - val_auc_pr: 0.8190 - val_auc_roc: 0.9269 - val_bin_acc: 0.8467 - val_loss: 0.2929 - val_precision: 0.6494 - val_recall: 0.7463 - learning_rate: 5.0000e-04\n",
            "Restoring model weights from the end of the best epoch: 7.\n",
            "\n",
            "🔓 Fase 2: fine-tuning parcial\n",
            "Epoch 1/12\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 638ms/step - auc_pr: 0.6690 - auc_roc: 0.8613 - bin_acc: 0.6860 - loss: 0.5951 - precision: 0.4275 - recall: 0.8859\n",
            "Epoch 1: val_loss improved from inf to 0.41500, saving model to ./dataset/best.keras\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 812ms/step - auc_pr: 0.6687 - auc_roc: 0.8612 - bin_acc: 0.6854 - loss: 0.5942 - precision: 0.4269 - recall: 0.8867 - val_auc_pr: 0.8172 - val_auc_roc: 0.9252 - val_bin_acc: 0.8432 - val_loss: 0.4150 - val_precision: 0.6100 - val_recall: 0.9104 - learning_rate: 2.0000e-05\n",
            "Epoch 2/12\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 637ms/step - auc_pr: 0.7874 - auc_roc: 0.9158 - bin_acc: 0.7626 - loss: 0.4004 - precision: 0.5123 - recall: 0.9513\n",
            "Epoch 2: val_loss did not improve from 0.41500\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 780ms/step - auc_pr: 0.7878 - auc_roc: 0.9161 - bin_acc: 0.7632 - loss: 0.3997 - precision: 0.5125 - recall: 0.9507 - val_auc_pr: 0.7847 - val_auc_roc: 0.9216 - val_bin_acc: 0.7735 - val_loss: 0.5897 - val_precision: 0.5082 - val_recall: 0.9254 - learning_rate: 2.0000e-05\n",
            "Epoch 3/12\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662ms/step - auc_pr: 0.8513 - auc_roc: 0.9467 - bin_acc: 0.8638 - loss: 0.3121 - precision: 0.6370 - recall: 0.9270\n",
            "Epoch 3: val_loss did not improve from 0.41500\n",
            "\n",
            "Epoch 3: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 788ms/step - auc_pr: 0.8517 - auc_roc: 0.9469 - bin_acc: 0.8640 - loss: 0.3117 - precision: 0.6376 - recall: 0.9268 - val_auc_pr: 0.7902 - val_auc_roc: 0.9230 - val_bin_acc: 0.7735 - val_loss: 0.6039 - val_precision: 0.5082 - val_recall: 0.9254 - learning_rate: 2.0000e-05\n",
            "Epoch 4/12\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 636ms/step - auc_pr: 0.8804 - auc_roc: 0.9643 - bin_acc: 0.8769 - loss: 0.2724 - precision: 0.6681 - recall: 0.9505\n",
            "Epoch 4: val_loss did not improve from 0.41500\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 743ms/step - auc_pr: 0.8807 - auc_roc: 0.9643 - bin_acc: 0.8769 - loss: 0.2723 - precision: 0.6680 - recall: 0.9504 - val_auc_pr: 0.7910 - val_auc_roc: 0.9235 - val_bin_acc: 0.7700 - val_loss: 0.6181 - val_precision: 0.5041 - val_recall: 0.9254 - learning_rate: 1.0000e-05\n",
            "Epoch 5/12\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 638ms/step - auc_pr: 0.9029 - auc_roc: 0.9667 - bin_acc: 0.8987 - loss: 0.2577 - precision: 0.7136 - recall: 0.9515\n",
            "Epoch 5: val_loss did not improve from 0.41500\n",
            "\n",
            "Epoch 5: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 782ms/step - auc_pr: 0.9032 - auc_roc: 0.9668 - bin_acc: 0.8986 - loss: 0.2574 - precision: 0.7132 - recall: 0.9517 - val_auc_pr: 0.7909 - val_auc_roc: 0.9252 - val_bin_acc: 0.7700 - val_loss: 0.5919 - val_precision: 0.5041 - val_recall: 0.9104 - learning_rate: 1.0000e-05\n",
            "Epoch 5: early stopping\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "\n",
            "✅ Modelo final: ./dataset/clf_validas_no_validas.keras\n",
            "🧠 Mejor checkpoint: ./dataset/best.keras\n",
            "📝 class_names.json guardado.\n",
            "\n",
            "📈 Evaluación final (positiva = no_validas) — validación estratificada\n",
            "🔎 AUC-PR(no_validas) = 0.819\n",
            "🔎 AUC-ROC(no_validas) = 0.925\n",
            "th=0.30 → Precision=0.517 | Recall=0.925 | F1=0.663\n",
            "th=0.40 → Precision=0.565 | Recall=0.910 | F1=0.697\n",
            "th=0.45 → Precision=0.592 | Recall=0.910 | F1=0.718\n",
            "th=0.50 → Precision=0.610 | Recall=0.910 | F1=0.731\n",
            "th=0.55 → Precision=0.621 | Recall=0.881 | F1=0.728\n",
            "th=0.60 → Precision=0.626 | Recall=0.851 | F1=0.722\n",
            "\n",
            "📋 Reporte (th=0.50, positiva=no_validas)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     validas      0.968     0.823     0.889       220\n",
            "  no_validas      0.610     0.910     0.731        67\n",
            "\n",
            "    accuracy                          0.843       287\n",
            "   macro avg      0.789     0.867     0.810       287\n",
            "weighted avg      0.884     0.843     0.852       287\n",
            "\n",
            "🧩 Matriz de confusión:\n",
            " [[181  39]\n",
            " [  6  61]]\n"
          ]
        }
      ],
      "source": [
        "# @title Entrenamiento de modelo por fases\n",
        "import os, zipfile, shutil, json, argparse, sys\n",
        "from typing import List, Tuple, Dict\n",
        "import glob\n",
        "import numpy as np\n",
        "from PIL import Image, UnidentifiedImageError, ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import BinaryAccuracy, Precision, Recall, AUC\n",
        "\n",
        "# configuración global (seed nos sirve para que al momento de re entrenar el modelo no nos dé resultados diferentes)\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "tf.keras.utils.set_random_seed(SEED)\n",
        "try:\n",
        "    from tensorflow.keras import mixed_precision\n",
        "    if tf.config.list_physical_devices('GPU'):\n",
        "        mixed_precision.set_global_policy(\"mixed_float16\"); print(\"⚙️ GPU: mixed_float16 ON\")\n",
        "    else:\n",
        "        mixed_precision.set_global_policy(\"float32\"); print(\"🧠 CPU: float32 ON\")\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "EXTS = {\".jpg\",\".jpeg\",\".png\",\".bmp\",\".webp\",\".tif\",\".tiff\",\".heic\",\".heif\"}\n",
        "\n",
        "# funciones auxiliares para lectura de carpetas e imágenes\n",
        "def clear_dir(path: str):\n",
        "    if os.path.exists(path):\n",
        "        shutil.rmtree(path)\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "def extract_zip_to_dir(zip_path: str, target_dir: str):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zf:\n",
        "        zf.extractall(target_dir)\n",
        "\n",
        "def gather_images(src_dir: str, dst_dir: str) -> int:\n",
        "    os.makedirs(dst_dir, exist_ok=True)\n",
        "    count = 0\n",
        "    for root, _, files_in in os.walk(src_dir):\n",
        "        if \"__MACOSX\" in root:\n",
        "            continue\n",
        "        for fname in files_in:\n",
        "            if fname.startswith(\"._\"):\n",
        "                continue\n",
        "            ext = os.path.splitext(fname)[1].lower()\n",
        "            if ext in EXTS:\n",
        "                src = os.path.join(root, fname)\n",
        "                new_name = f\"{count:08d}_{os.path.basename(fname)}\"\n",
        "                dst = os.path.join(dst_dir, new_name)\n",
        "                try:\n",
        "                    shutil.move(src, dst)\n",
        "                except Exception:\n",
        "                    try: shutil.copy2(src, dst)\n",
        "                    except Exception: continue\n",
        "                count += 1\n",
        "    return count\n",
        "\n",
        "def remove_too_small_files(folder: str, min_kb=5):\n",
        "    removed=0\n",
        "    for fname in list(os.listdir(folder)):\n",
        "        p=os.path.join(folder,fname)\n",
        "        if os.path.isfile(p):\n",
        "            try:\n",
        "                if os.path.getsize(p) < min_kb*1024:\n",
        "                    os.remove(p); removed+=1\n",
        "            except Exception:\n",
        "                pass\n",
        "    if removed>0:\n",
        "        print(f\"🧹 Eliminados <{min_kb}KB en {folder}: {removed}\")\n",
        "\n",
        "#unificar formatos si se encontraran varios\n",
        "def sanitize_and_convert_to_jpg(folder: str) -> Tuple[int,int,int]:\n",
        "    \"\"\"Convierte todo a JPG; las fallidas van a _fallidas/.\"\"\"\n",
        "    failed_dir = os.path.join(folder, \"_fallidas\"); os.makedirs(failed_dir, exist_ok=True)\n",
        "    ok, rewrote, fail, shown = 0, 0, 0, 0\n",
        "    for fname in list(os.listdir(folder)):\n",
        "        path = os.path.join(folder, fname)\n",
        "        if not os.path.isfile(path):\n",
        "            continue\n",
        "        root, ext = os.path.splitext(fname)\n",
        "        try:\n",
        "            with Image.open(path) as im:\n",
        "                if im.mode in (\"RGBA\",\"LA\"):\n",
        "                    bg = Image.new(\"RGB\", im.size, (255,255,255))\n",
        "                    bg.paste(im, mask=im.split()[-1]); im = bg\n",
        "                else:\n",
        "                    im = im.convert(\"RGB\")\n",
        "                out_path = os.path.join(folder, f\"{root}.jpg\")\n",
        "                im.save(out_path, format=\"JPEG\", quality=95, optimize=True)\n",
        "                if out_path != path:\n",
        "                    try: os.remove(path)\n",
        "                    except Exception: pass\n",
        "                if ext.lower() in (\".jpg\",\".jpeg\"):\n",
        "                    rewrote += 1\n",
        "                else:\n",
        "                    ok += 1\n",
        "        except (UnidentifiedImageError, OSError, ValueError) as e:\n",
        "            fail += 1\n",
        "            try: shutil.move(path, os.path.join(failed_dir, fname))\n",
        "            except Exception: pass\n",
        "            if shown < 8:\n",
        "                print(f\"⚠️ Falló {fname}: {type(e).__name__}: {e}\"); shown += 1\n",
        "    return ok, rewrote, fail\n",
        "\n",
        "def list_jpgs(dir_):\n",
        "    return [p for p in glob.glob(os.path.join(dir_, \"**\", \"*.jpg\"), recursive=True)\n",
        "            if \"/_fallidas/\" not in p and \"\\\\_fallidas\\\\\" not in p]\n",
        "\n",
        "def stratified_split(paths: List[str], val_frac=0.2, rng=None) -> Tuple[List[str], List[str]]:\n",
        "    rng = rng or np.random.RandomState(SEED)\n",
        "    arr = np.array(paths)\n",
        "    rng.shuffle(arr)\n",
        "    k = max(1, int(len(arr)*val_frac))\n",
        "    return arr[k:].tolist(), arr[:k].tolist()  # train, val\n",
        "\n",
        "def make_balanced_train(train_nv, train_v, mode=\"none\", rng=None):\n",
        "    rng = rng or np.random.RandomState(SEED)\n",
        "    train_nv = list(train_nv); train_v = list(train_v)\n",
        "    if mode == \"none\":\n",
        "        return train_nv, train_v\n",
        "    if mode == \"undersample\":\n",
        "        m = min(len(train_nv), len(train_v))\n",
        "        rng.shuffle(train_nv); rng.shuffle(train_v)\n",
        "        return train_nv[:m], train_v[:m]\n",
        "    if mode == \"oversample\":\n",
        "        if len(train_nv) < len(train_v):\n",
        "            need = len(train_v) - len(train_nv)\n",
        "            add = rng.choice(train_nv, size=need, replace=True).tolist()\n",
        "            return train_nv + add, train_v\n",
        "        else:\n",
        "            need = len(train_nv) - len(train_v)\n",
        "            add = rng.choice(train_v, size=need, replace=True).tolist()\n",
        "            return train_nv, train_v + add\n",
        "    if mode == \"both\":\n",
        "        target = max(len(train_nv), len(train_v))\n",
        "        if len(train_nv) < target:\n",
        "            add = rng.choice(train_nv, size=target-len(train_nv), replace=True).tolist()\n",
        "            train_nv = train_nv + add\n",
        "        if len(train_v) < target:\n",
        "            add = rng.choice(train_v, size=target-len(train_v), replace=True).tolist()\n",
        "            train_v = train_v + add\n",
        "        rng.shuffle(train_nv); rng.shuffle(train_v)\n",
        "        m = min(len(train_nv), len(train_v))\n",
        "        return train_nv[:m], train_v[:m]\n",
        "    raise ValueError(\"balance_mode inválido\")\n",
        "\n",
        "# datasets (positiva = no_validas -> etiqueta=1)\n",
        "def build_datasets_from_paths(train_nv, train_v, val_nv, val_v, img_size=(160,160), batch=32):\n",
        "    # labels: 1 = no_validas (positiva), 0 = validas\n",
        "    train_paths = np.array(list(train_nv) + list(train_v))\n",
        "    train_labels= np.array([1]*len(train_nv) + [0]*len(train_v))\n",
        "    val_paths   = np.array(list(val_nv) + list(val_v))\n",
        "    val_labels  = np.array([1]*len(val_nv) + [0]*len(val_v))\n",
        "\n",
        "    rng = np.random.RandomState(SEED)\n",
        "    p = rng.permutation(len(train_paths))\n",
        "    train_paths, train_labels = train_paths[p], train_labels[p]\n",
        "\n",
        "    def load_img(path, label):\n",
        "        img = tf.io.read_file(path)\n",
        "        img = tf.io.decode_jpeg(img, channels=3)\n",
        "        img = tf.image.resize(img, img_size)\n",
        "        return img, tf.cast(label, tf.int32)\n",
        "\n",
        "    def aug_fn(img, label):\n",
        "        img = tf.image.random_flip_left_right(img, seed=SEED)\n",
        "        img = tf.image.random_contrast(img, 0.9, 1.1, seed=SEED)\n",
        "        img = tf.image.random_brightness(img, 0.05, seed=SEED)\n",
        "        return img, label\n",
        "\n",
        "    def scale_fn(img, label):\n",
        "        # MobileNetV2 escala [-1,1]\n",
        "        img = (img/127.5) - 1.0\n",
        "        return img, label\n",
        "\n",
        "    train_ds = tf.data.Dataset.from_tensor_slices((train_paths, train_labels)) \\\n",
        "        .shuffle(2048, seed=SEED) \\\n",
        "        .map(load_img, num_parallel_calls=tf.data.AUTOTUNE) \\\n",
        "        .map(aug_fn, num_parallel_calls=tf.data.AUTOTUNE) \\\n",
        "        .map(scale_fn, num_parallel_calls=tf.data.AUTOTUNE) \\\n",
        "        .batch(batch).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    val_ds = tf.data.Dataset.from_tensor_slices((val_paths, val_labels)) \\\n",
        "        .map(load_img, num_parallel_calls=tf.data.AUTOTUNE) \\\n",
        "        .map(scale_fn, num_parallel_calls=tf.data.AUTOTUNE) \\\n",
        "        .batch(batch).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return train_ds, val_ds, (train_paths, train_labels, val_paths, val_labels)\n",
        "\n",
        "# MODELO\n",
        "def build_model(img_size=(160,160), try_imagenet=True):\n",
        "    inputs = layers.Input(shape=(*img_size, 3))\n",
        "\n",
        "    # escalado tf.data\n",
        "    from tensorflow.keras.applications import mobilenet_v2\n",
        "    base = None\n",
        "    if try_imagenet:\n",
        "        try:\n",
        "            print(\"🔼 Cargando MobileNetV2 (ImageNet)…\")\n",
        "            base = mobilenet_v2.MobileNetV2(include_top=False, weights=\"imagenet\", input_tensor=inputs, alpha=1.0)\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ No se pudo cargar ImageNet ({e}); entrenaré desde cero.\")\n",
        "    if base is None:\n",
        "        base = mobilenet_v2.MobileNetV2(include_top=False, weights=None, input_tensor=inputs, alpha=1.0)\n",
        "\n",
        "    base.trainable = False\n",
        "    x = layers.GlobalAveragePooling2D()(base.output)\n",
        "    x = layers.Dropout(0.35)(x)\n",
        "    out = layers.Dense(1, activation=\"sigmoid\", dtype=\"float32\", name=\"prob_no_validas\")(x)\n",
        "    model = models.Model(inputs, out)\n",
        "    return model, base\n",
        "\n",
        "def partial_unfreeze(base, fraction=0.30):\n",
        "    base.trainable = True\n",
        "    cut = int(len(base.layers)*(1.0 - fraction))\n",
        "    for i, layer in enumerate(base.layers):\n",
        "        layer.trainable = (i >= cut)\n",
        "\n",
        "def compute_class_weights(nv_count: int, v_count: int) -> Dict[int, float]:\n",
        "    total = nv_count + v_count\n",
        "    \"\"\" la etiqueta 1 es para imágenes no válidas y la etiqueta 0 para imágenes válidas\n",
        "        el cw se utiliza para balancear el peso del error según la proporción de imágenes con las que contamos\"\"\"\n",
        "    cw = {1: float(total / (2 * max(1, nv_count))),\n",
        "          0: float(total / (2 * max(1, v_count)))}\n",
        "    print(\"⚖️ class_weights:\", cw)\n",
        "    return cw\n",
        "\n",
        "# métricas y evaluación\n",
        "from sklearn.metrics import (classification_report, confusion_matrix,\n",
        "                             precision_score, recall_score, f1_score,\n",
        "                             average_precision_score, roc_auc_score)\n",
        "\n",
        "def evaluate(best_model, val_ds):\n",
        "    y_true, y_prob = [], []\n",
        "    for x, y in val_ds:\n",
        "        p = best_model.predict(x, verbose=0).reshape(-1)\n",
        "        y_true.append(y.numpy().reshape(-1))\n",
        "        y_prob.append(p)\n",
        "    y_true = np.concatenate(y_true).astype(int)\n",
        "    y_prob = np.concatenate(y_prob)\n",
        "\n",
        "    ap = average_precision_score(y_true, y_prob)\n",
        "    aucroc = roc_auc_score(y_true, y_prob)\n",
        "    print(f\"🔎 AUC-PR(no_validas) = {ap:.3f}\")\n",
        "    print(f\"🔎 AUC-ROC(no_validas) = {aucroc:.3f}\")\n",
        "\n",
        "    for th in [0.30, 0.40, 0.45, 0.50, 0.55, 0.60]:\n",
        "        yp = (y_prob >= th).astype(int)\n",
        "        p = precision_score(y_true, yp, zero_division=0)\n",
        "        r = recall_score(y_true, yp, zero_division=0)\n",
        "        f1= f1_score(y_true, yp, zero_division=0)\n",
        "        print(f\"th={th:.2f} → Precision={p:.3f} | Recall={r:.3f} | F1={f1:.3f}\")\n",
        "\n",
        "    yp = (y_prob >= 0.50).astype(int)\n",
        "    print(\"\\n📋 Reporte (th=0.50, positiva=no_validas)\")\n",
        "    print(classification_report(y_true, yp, target_names=[\"validas\",\"no_validas\"], digits=3))\n",
        "    print(\"🧩 Matriz de confusión:\\n\", confusion_matrix(y_true, yp))\n",
        "\n",
        "# función principal\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--mode\", choices=[\"colab\",\"local\"], default=\"colab\")\n",
        "    parser.add_argument(\"--validas_zip\", type=str, default=\"\")\n",
        "    parser.add_argument(\"--no_validas_zip\", type=str, default=\"\")\n",
        "    parser.add_argument(\"--data_root\", type=str, default=\"./dataset\")\n",
        "    parser.add_argument(\"--img_size\", type=int, default=160)\n",
        "    parser.add_argument(\"--batch\", type=int, default=32)\n",
        "    parser.add_argument(\"--val_frac\", type=float, default=0.20)\n",
        "    parser.add_argument(\"--balance_mode\", choices=[\"none\",\"undersample\",\"oversample\",\"both\"], default=\"none\")\n",
        "    parser.add_argument(\"--epochs_head\", type=int, default=8)\n",
        "    parser.add_argument(\"--epochs_ft\", type=int, default=12)\n",
        "    parser.add_argument(\"--lr_head\", type=float, default=1e-3)\n",
        "    parser.add_argument(\"--lr_ft\", type=float, default=2e-5)\n",
        "    parser.add_argument(\"--no_pretrained\", action=\"store_true\")\n",
        "    parser.add_argument(\"--export_saved_model\", action=\"store_true\")\n",
        "    args, _ = parser.parse_known_args()\n",
        "\n",
        "    DATA_ROOT = args.data_root\n",
        "    IMG_SIZE = (args.img_size, args.img_size)\n",
        "\n",
        "    # Preparar carpetas\n",
        "    clear_dir(DATA_ROOT)\n",
        "    vdir = os.path.join(DATA_ROOT, \"validas\")\n",
        "    nvdir = os.path.join(DATA_ROOT, \"no_validas\")\n",
        "    os.makedirs(vdir, exist_ok=True); os.makedirs(nvdir, exist_ok=True)\n",
        "\n",
        "    # Ingesta\n",
        "    if args.mode == \"colab\":\n",
        "        try:\n",
        "            from google.colab import files\n",
        "        except Exception:\n",
        "            print(\"❌ No estás en Colab. Usa --mode local.\"); sys.exit(1)\n",
        "        print(\"📦 Sube el ZIP para *validas*\")\n",
        "        up_v = files.upload(); vz = next(iter(up_v.keys()))\n",
        "        print(\"📦 Sube el ZIP para *no_validas*\")\n",
        "        up_nv = files.upload(); nvz = next(iter(up_nv.keys()))\n",
        "    else:\n",
        "        if not (args.validas_zip and args.no_validas_zip):\n",
        "            print(\"❌ En modo local pasa --validas_zip y --no_validas_zip\"); sys.exit(1)\n",
        "        vz, nvz = args.validas_zip, args.no_validas_zip\n",
        "\n",
        "    # Extraer y consolidar\n",
        "    tmp_v, tmp_nv = \"./tmp_validas\", \"./tmp_no_validas\"\n",
        "    clear_dir(tmp_v); clear_dir(tmp_nv)\n",
        "    extract_zip_to_dir(vz, tmp_v); extract_zip_to_dir(nvz, tmp_nv)\n",
        "    n_v = gather_images(tmp_v, vdir); n_nv = gather_images(tmp_nv, nvdir)\n",
        "    shutil.rmtree(tmp_v, ignore_errors=True); shutil.rmtree(tmp_nv, ignore_errors=True)\n",
        "    print(f\"📦 validas movidas: {n_v}  |  no_validas movidas: {n_nv}\")\n",
        "\n",
        "    # Filtrar ruido y sanitizar\n",
        "    remove_too_small_files(vdir, 5); remove_too_small_files(nvdir, 5)\n",
        "    print(\"🧼 Sanitizando 'validas'…\")\n",
        "    ok_v, rew_v, fail_v = sanitize_and_convert_to_jpg(vdir)\n",
        "    print(f\"   ✔ convertidos: {ok_v}, re-salvados jpg: {rew_v}, fallidos: {fail_v}\")\n",
        "    print(\"🧼 Sanitizando 'no_validas'…\")\n",
        "    ok_nv, rew_nv, fail_nv = sanitize_and_convert_to_jpg(nvdir)\n",
        "    print(f\"   ✔ convertidos: {ok_nv}, re-salvados jpg: {rew_nv}, fallidos: {fail_nv}\")\n",
        "\n",
        "    # Listado y split estratificado\n",
        "    paths_nv = list_jpgs(nvdir)\n",
        "    paths_v  = list_jpgs(vdir)\n",
        "    print(f\"📊 Dataset → no_validas={len(paths_nv)} | validas={len(paths_v)}\")\n",
        "\n",
        "    train_nv, val_nv = stratified_split(paths_nv, args.val_frac)\n",
        "    train_v,  val_v  = stratified_split(paths_v,  args.val_frac)\n",
        "    print(f\"✂️ Split → train: no_validas={len(train_nv)} validas={len(train_v)} | val: no_validas={len(val_nv)} validas={len(val_v)}\")\n",
        "\n",
        "    train_nv_b, train_v_b = make_balanced_train(train_nv, train_v, args.balance_mode)\n",
        "    print(f\"⚖️ Train balanceado ({args.balance_mode}) → no_validas={len(train_nv_b)} | validas={len(train_v_b)}\")\n",
        "\n",
        "    # Datasets\n",
        "    train_ds, val_ds, _ = build_datasets_from_paths(train_nv_b, train_v_b, val_nv, val_v,\n",
        "                                                    img_size=IMG_SIZE, batch=args.batch)\n",
        "\n",
        "    # Modelo\n",
        "    model, backbone = build_model(IMG_SIZE, try_imagenet=not args.no_pretrained)\n",
        "\n",
        "    loss = tf.keras.losses.BinaryCrossentropy()\n",
        "    metrics = [BinaryAccuracy(name=\"bin_acc\"),\n",
        "               Precision(name=\"precision\"),\n",
        "               Recall(name=\"recall\"),\n",
        "               AUC(name=\"auc_roc\", curve=\"ROC\"),\n",
        "               AUC(name=\"auc_pr\", curve=\"PR\")]\n",
        "\n",
        "    callbacks_head = [\n",
        "        tf.keras.callbacks.ModelCheckpoint(os.path.join(DATA_ROOT, \"best.keras\"), monitor=\"val_loss\", save_best_only=True, verbose=1),\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, min_lr=1e-6, verbose=1),\n",
        "        tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True, verbose=1),\n",
        "    ]\n",
        "\n",
        "    # class_weights solo si no balanceamos\n",
        "    use_cw = (args.balance_mode == \"none\")\n",
        "    class_weights = None\n",
        "    if use_cw:\n",
        "        class_weights = compute_class_weights(len(train_nv), len(train_v))\n",
        "\n",
        "    # Fase 1\n",
        "    print(\"\\n🚀 Fase 1: entrenando cabezal (backbone congelado)\")\n",
        "    model.compile(optimizer=Adam(args.lr_head), loss=loss, metrics=metrics)\n",
        "    model.fit(train_ds, validation_data=val_ds, epochs=args.epochs_head,\n",
        "              callbacks=callbacks_head, class_weight=class_weights)\n",
        "\n",
        "    # Fase 2\n",
        "    print(\"\\n🔓 Fase 2: fine-tuning parcial\")\n",
        "    partial_unfreeze(backbone, fraction=0.30)\n",
        "    callbacks_ft = [\n",
        "        tf.keras.callbacks.ModelCheckpoint(os.path.join(DATA_ROOT, \"best.keras\"), monitor=\"val_loss\", save_best_only=True, verbose=1),\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, min_lr=1e-6, verbose=1),\n",
        "        tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=4, restore_best_weights=True, verbose=1),\n",
        "    ]\n",
        "    model.compile(optimizer=Adam(args.lr_ft), loss=loss, metrics=metrics)\n",
        "    model.fit(train_ds, validation_data=val_ds, epochs=args.epochs_ft,\n",
        "              callbacks=callbacks_ft, class_weight=class_weights)\n",
        "\n",
        "    # Guardados\n",
        "    with open(os.path.join(DATA_ROOT, \"class_names.json\"), \"w\") as f:\n",
        "        json.dump([\"no_validas\",\"validas\"], f)\n",
        "    out_model = os.path.join(DATA_ROOT, \"clf_validas_no_validas.keras\")\n",
        "    model.save(out_model)\n",
        "    print(f\"\\n✅ Modelo final: {out_model}\")\n",
        "    print(f\"🧠 Mejor checkpoint: {os.path.join(DATA_ROOT, 'best.keras')}\")\n",
        "    print(f\"📝 class_names.json guardado.\")\n",
        "\n",
        "    # SavedModel opcional\n",
        "    if args.export_saved_model:\n",
        "        export_dir = os.path.join(DATA_ROOT, \"saved_model\")\n",
        "        try:\n",
        "            @tf.function(input_signature=[tf.TensorSpec([None, *IMG_SIZE, 3], tf.float32)])\n",
        "            def serving_fn(x): return {\"prob_no_validas\": model(x, training=False)}\n",
        "            tf.saved_model.save(model, export_dir, signatures={\"serving_default\": serving_fn})\n",
        "            print(f\"📦 SavedModel exportado en: {export_dir}\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ No se pudo exportar SavedModel: {e}\")\n",
        "\n",
        "    # Evaluación final\n",
        "    print(\"\\n📈 Evaluación final (positiva = no_validas) — validación estratificada\")\n",
        "    best = tf.keras.models.load_model(os.path.join(DATA_ROOT, \"best.keras\"))\n",
        "    evaluate(best, val_ds)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"dataset/best.keras\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "PcHat4Pe-QSO",
        "outputId": "6ec2e59b-3da8-48e6-a8c8-cf5eae3c0baa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9816c422-a78c-4d9f-bd81-6fc579a0c91c\", \"best.keras\", 24063236)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Clasificador\n",
        "import os, zipfile, shutil, json, argparse, sys, csv\n",
        "from typing import List, Tuple\n",
        "import numpy as np\n",
        "\n",
        "from PIL import Image, UnidentifiedImageError, ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# configuración por defecto del modelo\n",
        "MODEL_PATH_DEFAULT = \"./dataset/best.keras\"\n",
        "CLASSES_JSON_DEFAULT = \"./dataset/class_names.json\"\n",
        "\n",
        "# Como queremos menos falsos positivos, ponemos un umbral alto por defecto.\n",
        "LOW_DEFAULT  = 0.50     # este no se usa ahorita porque no queremos crear carpeta revisar, pero por si acaso, también está aquí\n",
        "HIGH_DEFAULT = 0.75     # >= HIGH => no_validas; < HIGH => validas\n",
        "USE_GRAY_ZONE_DEFAULT = False  # sin zona gris (esto sirve para la carpeta revisar)\n",
        "\n",
        "\n",
        "def clear_dir(path: str):\n",
        "    if os.path.exists(path):\n",
        "        shutil.rmtree(path)\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "def extract_zip_to_dir(zip_path: str, target_dir: str):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zf:\n",
        "        zf.extractall(target_dir)\n",
        "\n",
        "def iter_image_files(root_dir: str, exts={\".jpg\",\".jpeg\",\".png\",\".bmp\",\".webp\",\".tif\",\".tiff\",\".heic\",\".heif\"}):\n",
        "    for root, _, fs in os.walk(root_dir):\n",
        "        if \"__MACOSX\" in root:  # esto es por si está usando una macbook\n",
        "            continue\n",
        "        for fname in fs:\n",
        "            if fname.startswith(\"._\"):\n",
        "                continue\n",
        "            ext = os.path.splitext(fname)[1].lower()\n",
        "            if ext in exts:\n",
        "                yield os.path.join(root, fname)\n",
        "\n",
        "def load_img_arr(path: str, size: Tuple[int,int]):\n",
        "    # Devuelve array float32 escalado a [-1,1] (MobileNetV2).\n",
        "    with Image.open(path) as im:\n",
        "        if im.mode in (\"RGBA\",\"LA\"):\n",
        "            bg = Image.new(\"RGB\", im.size, (255,255,255))\n",
        "            bg.paste(im, mask=im.split()[-1])\n",
        "            im = bg\n",
        "        else:\n",
        "            im = im.convert(\"RGB\")\n",
        "        im = im.resize(size, resample=Image.BILINEAR)\n",
        "        arr = np.asarray(im, dtype=np.float32)\n",
        "    arr = (arr / 127.5) - 1.0\n",
        "    return arr\n",
        "\n",
        "def safe_makedirs(path: str):\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "def write_csv(rows: List[Tuple[str,float,str]], out_csv: str):\n",
        "    with open(out_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        w = csv.writer(f)\n",
        "        w.writerow([\"file\", \"prob_no_validas\", \"decision\"])\n",
        "        w.writerows(rows)\n",
        "\n",
        "# función principal\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--mode\", choices=[\"colab\",\"local\"], default=\"colab\")\n",
        "    parser.add_argument(\"--input_zip\", type=str, default=\"\", help=\"Ruta al ZIP de imágenes (modo local)\")\n",
        "    parser.add_argument(\"--model_path\", type=str, default=MODEL_PATH_DEFAULT)\n",
        "    parser.add_argument(\"--classes_json\", type=str, default=CLASSES_JSON_DEFAULT)\n",
        "    parser.add_argument(\"--img_size\", type=int, default=160)  # usa 160 si entrenaste a 160; 224 si a 224\n",
        "    parser.add_argument(\"--batch\", type=int, default=64)\n",
        "\n",
        "    # Umbral binario\n",
        "    parser.add_argument(\"--high\", type=float, default=HIGH_DEFAULT, help=\"Umbral binario: >= high => no_validas\")\n",
        "\n",
        "    # aquí no se está usando gray zone (que es la establece imágenes confusas en carpeta revisar)\n",
        "    parser.add_argument(\"--use_gray_zone\", action=\"store_true\", default=USE_GRAY_ZONE_DEFAULT)\n",
        "    parser.add_argument(\"--low\", type=float, default=LOW_DEFAULT)\n",
        "\n",
        "    # Salidas\n",
        "    parser.add_argument(\"--workdir\", type=str, default=\"./clasificacion_work\")\n",
        "    parser.add_argument(\"--output_zip\", type=str, default=\"./clasificado.zip\")\n",
        "    parser.add_argument(\"--output_csv\", type=str, default=\"./resultados.csv\")\n",
        "\n",
        "    args, _ = parser.parse_known_args()\n",
        "\n",
        "    # leer zip de imágenes\n",
        "    IN_DIR = os.path.join(args.workdir, \"imagenes_input\")\n",
        "    clear_dir(args.workdir)\n",
        "    safe_makedirs(IN_DIR)\n",
        "\n",
        "    if args.mode == \"colab\":\n",
        "        try:\n",
        "            from google.colab import files as colab_files  # type: ignore\n",
        "        except Exception:\n",
        "            print(\"❌ No estás en Colab; usa --mode local y --input_zip\"); sys.exit(1)\n",
        "        print(\"📦 Sube el archivo ZIP con imágenes a clasificar\")\n",
        "        uploaded = colab_files.upload()\n",
        "        if not uploaded:\n",
        "            print(\"❌ No se subió nada.\"); sys.exit(1)\n",
        "        input_zip = next(iter(uploaded.keys()))\n",
        "    else:\n",
        "        if not args.input_zip or not os.path.exists(args.input_zip):\n",
        "            print(\"❌ En modo local debes pasar --input_zip con ruta\")\n",
        "            sys.exit(1)\n",
        "        input_zip = args.input_zip\n",
        "\n",
        "    extract_zip_to_dir(input_zip, IN_DIR)\n",
        "\n",
        "    # recolecta imágenes\n",
        "    files_list = sorted(list(iter_image_files(IN_DIR)))\n",
        "    print(f\"📂 Total archivos candidatos: {len(files_list)}\")\n",
        "    if len(files_list) == 0:\n",
        "        print(\"❌ No se encontraron imágenes válidas dentro del ZIP.\"); sys.exit(1)\n",
        "\n",
        "    # cargar modelo\n",
        "    print(\"🔮 Cargando modelo…\")\n",
        "    model = tf.keras.models.load_model(args.model_path)\n",
        "    with open(args.classes_json, \"r\") as f:\n",
        "        class_names = json.load(f)\n",
        "    # probabilidad de no válidas\n",
        "    if isinstance(class_names, list) and \"no_validas\" in class_names:\n",
        "        idx_no_validas = class_names.index(\"no_validas\")\n",
        "    else:\n",
        "        idx_no_validas = 0\n",
        "    print(\"✅ Modelo cargado.\")\n",
        "\n",
        "    IMG_SIZE = (args.img_size, args.img_size)\n",
        "    BATCH = args.batch\n",
        "    probs, paths_ok, failed = [], [], []\n",
        "\n",
        "    # inferencia\n",
        "    print(\"🧠 Ejecutando inferencia…\")\n",
        "    for start in range(0, len(files_list), BATCH):\n",
        "        batch = files_list[start:start+BATCH]\n",
        "        x_batch, keep = [], []\n",
        "        for p in batch:\n",
        "            try:\n",
        "                x_batch.append(load_img_arr(p, IMG_SIZE))\n",
        "                keep.append(p)\n",
        "            except (UnidentifiedImageError, OSError, ValueError) as e:\n",
        "                failed.append((p, type(e).__name__))\n",
        "        if not x_batch:\n",
        "            continue\n",
        "        x = np.stack(x_batch, axis=0)\n",
        "        pred = model.predict(x, verbose=0)\n",
        "        pred = pred.reshape(-1).tolist()\n",
        "        probs.extend(pred)\n",
        "        paths_ok.extend(keep)\n",
        "\n",
        "    print(f\"✅ Inferencia terminada. Exitosas: {len(paths_ok)}  |  Fallidas al leer: {len(failed)}\")\n",
        "\n",
        "    # decisión\n",
        "    HIGH = args.high\n",
        "    def decide(p):\n",
        "        return \"no_validas\" if p >= HIGH else \"validas\"\n",
        "\n",
        "    decisions = [decide(p) for p in probs]\n",
        "\n",
        "    # clasificación en carpetas y descarga de zip\n",
        "    OUT_DIR = os.path.join(args.workdir, \"salida\")\n",
        "    dir_validas   = os.path.join(OUT_DIR, \"validas\")\n",
        "    dir_novalidas = os.path.join(OUT_DIR, \"no_validas\")\n",
        "    safe_makedirs(dir_validas); safe_makedirs(dir_novalidas)\n",
        "\n",
        "    rows_csv = []\n",
        "    counts = {\"validas\":0,\"no_validas\":0}\n",
        "    for p, pr, lab in zip(paths_ok, probs, decisions):\n",
        "        dst_dir = dir_validas if lab==\"validas\" else dir_novalidas\n",
        "        dst_path = os.path.join(dst_dir, os.path.basename(p))\n",
        "        try:\n",
        "            shutil.copy2(p, dst_path)\n",
        "        except Exception:\n",
        "            with open(p, \"rb\") as fsrc, open(dst_path, \"wb\") as fdst:\n",
        "                fdst.write(fsrc.read())\n",
        "        rows_csv.append((os.path.relpath(dst_path, OUT_DIR), float(pr), lab))\n",
        "        counts[lab] = counts.get(lab, 0) + 1\n",
        "\n",
        "    # guardar un csv con los resultados\n",
        "    write_csv(rows_csv, args.output_csv)\n",
        "\n",
        "    # crear un zip al final\n",
        "    if os.path.exists(args.output_zip):\n",
        "        os.remove(args.output_zip)\n",
        "    with zipfile.ZipFile(args.output_zip, \"w\", zipfile.ZIP_DEFLATED) as zf:\n",
        "        for root,_,fs in os.walk(OUT_DIR):\n",
        "            for f in fs:\n",
        "                full = os.path.join(root, f)\n",
        "                rel = os.path.relpath(full, OUT_DIR)\n",
        "                zf.write(full, rel)\n",
        "\n",
        "    print(\"\\n📊 Resumen de clasificación\")\n",
        "    print(f\"  - validas:    {counts.get('validas',0)}\")\n",
        "    print(f\"  - no_validas: {counts.get('no_validas',0)}\")\n",
        "    if failed:\n",
        "        print(f\"  - fallidas al leer: {len(failed)} (omitidas)\")\n",
        "\n",
        "    print(f\"\\n✅ ZIP listo: {args.output_zip}\")\n",
        "    print(f\"✅ CSV listo: {args.output_csv}\")\n",
        "    if args.mode == \"colab\":\n",
        "        from google.colab import files as colab_files\n",
        "        colab_files.download(args.output_zip)\n",
        "        colab_files.download(args.output_csv)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "cellView": "form",
        "id": "IMz2kqDHgxaE",
        "outputId": "4975d50e-22e0-46da-c073-2e809043366b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📦 Sube tu archivo ZIP con imágenes a clasificar\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ddd1873a-eca7-40d1-b059-0bb6215f0f16\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ddd1873a-eca7-40d1-b059-0bb6215f0f16\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving ImagesTest.zip to ImagesTest.zip\n",
            "📂 Total archivos candidatos: 1768\n",
            "🔮 Cargando modelo…\n",
            "✅ Modelo cargado.\n",
            "🧠 Ejecutando inferencia…\n",
            "✅ Inferencia terminada. Exitosas: 1768  |  Fallidas al leer: 0\n",
            "\n",
            "📊 Resumen de clasificación\n",
            "  - validas:    1298\n",
            "  - no_validas: 470\n",
            "\n",
            "✅ ZIP listo: ./clasificado.zip\n",
            "✅ CSV listo: ./resultados.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a2084611-7e2b-42bc-9e81-7e46b8cb5469\", \"clasificado.zip\", 161894547)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_4ce3fc05-bf48-4085-bc7c-f224f1737a44\", \"resultados.csv\", 94072)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}